

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>8.1. Lecture 21 &#8212; Learning from data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "alphavec": ["\\boldsymbol{\\alpha}"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "fvec": ["\\boldsymbol{f}"], "mvec": ["\\boldsymbol{m}"], "qvec": ["\\boldsymbol{q}"], "rvec": ["\\boldsymbol{r}"], "uvec": ["\\boldsymbol{u}"], "wvec": ["\\boldsymbol{w}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"], "gs": ["{0}"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/Maximum_entropy/lecture_21';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="8.2. Ignorance pdfs: Indifference and translation groups" href="../../notebooks/Maximum_entropy/MaxEnt.html" />
    <link rel="prev" title="8. Maximum entropy" href="max_ent.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../about.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/buqeye_logo_web.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/buqeye_logo_web.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../about.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Course/overview.html">Objectives</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topics</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Basics/basics.html">1. Basics of Bayesian statistics</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Basics/lecture_01.html">1.1. Lecture 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/Exploring_pdfs.html">1.2. Exploring PDFs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule.html">1.3. Checking the sum and product rules, and their consequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Basics/lecture_02.html">1.4. Lecture 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/Bayesian_updating_coinflip_interactive.html">1.5. Interactive Bayesian updating: coin flipping example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes.html">1.6. Standard medical example by applying Bayesian rules of probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise.html">1.7. Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Basics/lecture_03.html">1.8. Lecture 3</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Parameter_estimation/param_est.html">2. Bayesian parameter estimation</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Parameter_estimation/lecture_04.html">2.1. Lecture 4: Parameter estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise.html">2.2. Parameter estimation example: Gaussian noise and averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/assignments/Assignment_extending_radioactive_lighthouse.html">2.3. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Parameter_estimation/lecture_05.html">2.4. Lecture 5</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">2.5. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/demo-ModelValidation.html">2.6. Linear Regression and Model Validation demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Parameter_estimation/lecture_06.html">2.7. Lecture 6</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/amplitude_in_presence_of_background.html">2.8. Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/assignments/Assignment_parameter_estimation_followups.html">2.9. Assignment: Follow-ups to Parameter Estimation notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/exercise_LinearRegression.html">2.10. Linear Regression exercise</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/linear_algebra_games_I.html">2.11. Linear algebra games including SVD for PCA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">2.12. Follow-up: fluctuation trends with # of points and data errors</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MCMC_sampling_I/MCMC_sampling_I.html">3. MCMC sampling I</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_I/lecture_07.html">3.1. Lecture 7</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_I/Metropolis_Poisson_example.html">3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_I/lecture_08.html">3.3. Lecture 8</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-random-walk-and-sampling.html">3.4. Exercise: Random walk</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Why_Bayes_is_better/bayes_is_better.html">4. Why Bayes is better</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_09.html">4.1. Lecture 9</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/bayes_billiard.html">4.2. A Bayesian Billiard game</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_10.html">4.3. Lecture 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">4.4. Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_11.html">4.5. Lecture 11</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">4.6. Error propagation: Example 3.6.2 in Sivia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/visualization_of_CLT.html">4.7. Visualization of the Central Limit Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/correlation_intuition.html">4.8. Building intuition about correlations (and a bit of Python linear algebra)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_12.html">4.9. Lecture 12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-diagnostics.html">4.10. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_13.html">4.12. Lecture 13</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/dealing_with_outliers.html">4.13. Dealing with outliers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Model_selection/model_selection.html">5. Model selection</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Model_selection/lecture_14.html">5.1. Lecture 14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Model_selection/lecture_15.html">5.2. Lecture 15</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Model_selection/Evidence_for_model_EFT_coefficients.html">5.3. Evidence calculation for EFT expansions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Model_selection/lecture_16.html">5.4. Lecture 16</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee.html">5.5. Example: Parallel tempering for multimodal distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee_vs_zeus.html">5.6. Example: Parallel tempering for multimodal distributions vs. zeus</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MCMC_sampling_II/MCMC_sampling_II.html">6. MCMC sampling II</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_II/lecture_17.html">6.1. Lecture 17</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/chi_squared_tests.html">6.2. Quick check of the distribution of normal variables squared</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/Liouville_theorem_visualization.html">6.3. Liouville Theorem Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">6.4. Solving orbital equations with different algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_II/lecture_18.html">6.5. Lecture 18</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_intro_updated.html">6.6. PyMC3 Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_docs_getting_started_updated.html">6.7. Getting started with PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise_compare_samplers.html">6.8. Comparing samplers for a simple problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/mini-projects/zeus_multimodal.html">6.9. zeus: Sampling from multimodal distributions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Gaussian_processes/gaussian_processes.html">7. Gaussian processes</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Gaussian_processes/lecture_19.html">7.1. Lecture 19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Gaussian_processes/demo-GaussianProcesses.html">7.2. Gaussian processes demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Gaussian_processes/GaussianProcesses.html">7.3. Learning from data: Gaussian processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Gaussian_processes/Gaussian_processes_exercises.html">7.4. Exercise: Gaussian Process models with GPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Gaussian_processes/lecture_20.html">7.5. Lecture 20</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="max_ent.html">8. Assigning probabilities</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">8.1. Lecture 21</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html">8.2. Ignorance pdfs: Indifference and translation groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/Pdfs_from_MaxEnt.html">8.3. MaxEnt for deriving some probability distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html">8.4. Maximum Entropy for reconstructing a function from its moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/demo-MaxEnt.html">8.5. Making figures for Ignorance PDF notebook</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Machine_learning/machine_learning.html">9. Machine learning: Bayesian methods</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Machine_learning/lecture_22.html">9.1. Lecture 22</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_optimization.html">9.2. Bayesian Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Machine_learning/lecture_23.html">9.3. Lecture 23</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Neural_networks_explained.html">9.4. What Are Neural Networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_NeuralNet.html">9.5. Neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_demo-NeuralNet.html">9.6. Neural network classifier demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_neural_networks_tif285.html">9.7. Bayesian neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Machine_learning/lecture_24.html">9.8. Lecture 24</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/demo-Bayesian_neural_networks_tif285.html">9.9. Variational Inference: Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Convolutional_neural_network_explained.html">9.10. What is a convolutional neural network?</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../SVD/svd.html">10. PCA, SVD, and all that</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../SVD/lecture_25.html">10.1. Lecture 25</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/SVD/linear_algebra_games_including_SVD.html">10.2. Linear algebra games including SVD for PCA</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/mini-project_I_toy_model_of_EFT.html">Mini-project I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIa.html">Mini-project IIa: Model selection basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">Mini-project IIb: How many lines are there?</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIa_bayesian_optimization.html">Mini-project IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">Mini-project IIIb: Bayesian Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference material</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../zbibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../related_topics.html">Related topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/installing_anaconda.html">Using Anaconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/using_github.html">Using GitHub</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Reference/python_jupyter.html">Python and Jupyter notebooks</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_01.html">Python and Jupyter notebooks: part 01</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_02.html">Python and Jupyter notebooks: part 02</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../jb_tests.html">Examples: Jupyter jb-book</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notebook keys</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule_KEY.html">Checking the sum and product rules, and their consequences <span style="color: red">Key</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes_KEY.html">Standard medical example by applying Bayesian rules of probability <span style="color: red">Key</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise_key.html">Radioactive lighthouse problem  <span style="color: red">Key</span></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/DanielRPhillips/LearningFromData" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DanielRPhillips/LearningFromData/issues/new?title=Issue%20on%20page%20%2Fcontent/Maximum_entropy/lecture_21.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/Maximum_entropy/lecture_21.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 21</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#follow-up-to-gaussian-process-exercises">Follow-up to Gaussian process exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-entropy">Maximum entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principle-of-maximum-entropy">Principle of Maximum Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-gaussian-distribution">Example 1: Gaussian distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-poisson-distributions">Example 2: Poisson distributions</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-21">
<h1><span class="section-number">8.1. </span>Lecture 21<a class="headerlink" href="#lecture-21" title="Permalink to this heading">#</a></h1>
<section id="follow-up-to-gaussian-process-exercises">
<h2>Follow-up to Gaussian process exercises<a class="headerlink" href="#follow-up-to-gaussian-process-exercises" title="Permalink to this heading">#</a></h2>
<p>Return to the notebook <a class="reference internal" href="../../notebooks/Gaussian_processes/Gaussian_processes_exercises.html"><span class="doc std std-doc">Exercise: Gaussian Process models with GPy</span></a>.</p>
<ul class="simple">
<li><p>Do sampling of different covariant functions in <em>2 Sampling from a Gaussian Process</em>.</p>
<ul>
<li><p>Predict <code class="docutils literal notranslate"><span class="pre">nsamples</span> <span class="pre">=</span> <span class="pre">50</span></code>.</p></li>
</ul>
</li>
<li><p>Try some combinations</p>
<ul>
<li><p>linear <span class="math notranslate nohighlight">\(\longrightarrow\)</span> polynomial (try two to get quadratic). [See kernels.pdf Fig 1.1 (on page 2) and Fig. 1.2 (on page 4).]</p></li>
</ul>
</li>
<li><p>For Gaussian Process Regression Model</p>
<ul>
<li><p>distinguish between noise in <em>data</em> and noise in <em>model</em></p></li>
<li><p>compare the true function in red to the curves</p></li>
</ul>
</li>
<li><p>Other things you might play with:</p>
<ul>
<li><p>Add a function for the true result (no noise) and add it (in red) to the plots.</p></li>
<li><p>Compare small data noise vs. large data noise.</p></li>
<li><p>Try making lengthscale very small <span class="math notranslate nohighlight">\(\Lra\)</span> explain the result. [Should return to the mean after a few length scales.]</p></li>
<li><p>Try optimizing with lengthscale very small (it doesn’t change <span class="math notranslate nohighlight">\(\Lra\)</span> optimize fails).</p></li>
<li><p>With a good optimization, explore how well red line is withing the 95% bands.</p>
<ul>
<li><p>relation to prior (mean zero, input variance)</p></li>
<li><p>What if I extend the range of <code class="docutils literal notranslate"><span class="pre">Xtrue</span></code>?</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="maximum-entropy">
<h2>Maximum entropy<a class="headerlink" href="#maximum-entropy" title="Permalink to this heading">#</a></h2>
<p>A good reference here is Chapter 5 in Sivia <span id="id1">[<a class="reference internal" href="../zbibliography.html#id9" title="D. S. Sivia and J. Skilling. Data Analysis - A Bayesian Tutorial. Oxford Science Publications. Oxford University Press, 2nd edition, 2006.">SS06</a>]</span>.
The plan is to step through <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html"><span class="doc std std-doc">Ignorance pdfs: Indifference and translation groups</span></a>, then <a class="reference internal" href="../../notebooks/Maximum_entropy/Pdfs_from_MaxEnt.html"><span class="doc std std-doc">MaxEnt for deriving some probability distributions</span></a> as a class exercise. As time permits, we’ll do <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html"><span class="doc std std-doc">Maximum Entropy for reconstructing a function from its moments</span></a>.</p>
<p>Notes on <a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html"><span class="doc std std-doc">Ignorance pdfs: Indifference and translation groups</span></a>:</p>
<ul>
<li><p>Ignorance pdfs <span class="math notranslate nohighlight">\(\longrightarrow\)</span> when we don’t have constraints or extra knowledge that breaks symmetries.</p>
<ol class="arabic">
<li><p>permutation symmetry <span class="math notranslate nohighlight">\(\longrightarrow\)</span> die <span class="math notranslate nohighlight">\(\Lra\)</span>
1/(# choices) [discrete]</p></li>
<li><p>translational invariance <span class="math notranslate nohighlight">\(\longrightarrow\)</span> <span class="math notranslate nohighlight">\(p(x|I) = \text{constant}\)</span> (in allowed region)</p></li>
<li><p>scale invariance <span class="math notranslate nohighlight">\(\longrightarrow\)</span> <span class="math notranslate nohighlight">\(p(x|I) \propto 1/x\)</span></p>
<ul>
<li><p>How to derive?</p></li>
<li><p>First check that it works: <span class="math notranslate nohighlight">\(p(x|I) = \lambda p(\lambda x|I)\)</span> <span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(\frac{c}{x} = \frac{\lambda c}{\lambda x} = \frac{c}{x}\)</span>. Check!</p></li>
<li><p>Now more general proof: assume <span class="math notranslate nohighlight">\(p(x|I) \propto x^{\alpha}\)</span>
<span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(x^\alpha = \lambda (\lambda x)^{\alpha}
= \lambda^{1+\alpha} x^\alpha\)</span> <span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(\alpha = -1\)</span>. Check!</p></li>
<li><p>Still more general: set <span class="math notranslate nohighlight">\(\lambda = 1 + \epsilon\)</span> with <span class="math notranslate nohighlight">\(\epsilon \ll 1\)</span>, and solve to <span class="math notranslate nohighlight">\(\mathcal{O}(\epsilon)\)</span>: <span class="math notranslate nohighlight">\(p(x) = (1+\epsilon)(p(x)+\epsilon\frac{dp}{dx})\)</span> <span class="math notranslate nohighlight">\(\Lra\)</span> <span class="math notranslate nohighlight">\(p(x) + x \frac{dp}{dx} = 0\)</span></p>
<div class="math notranslate nohighlight">
\[
             \Lra \int_{p(x_0)}^{p(x)} \frac{dp}{p}
             = \int_{x_0}^x \frac{dx'}{x'}
             \ \Lra\ 
             \log\frac{p(x)}{p(x_0)} = \log\frac{x_0}{x}
             \ \Lra\  p(x) = \left(\frac{p(x_0)}{x_0}\right)\frac{1}{x}
            \]</div>
<p>so <span class="math notranslate nohighlight">\(p(x) \propto 1/x\)</span>.</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>Step quickly through Symmetry invariance.</p>
<ul class="simple">
<li><p>Basically using a change of variables for the symmetry, which means a Jacobian.</p></li>
<li><p>For the linear model: <span class="math notranslate nohighlight">\(y_{\rm th}(x) = \theta_1 x + \theta_0\)</span>, which we could write the other way around: <span class="math notranslate nohighlight">\(x_{\rm th}(y) = \theta'_1 y + \theta'_0\)</span>, and these probabilities (not densities!) should be equal:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
     p(\theta_0,\theta_1|I) d\theta_0 d\theta_1
       = p(\theta'_0,\theta'_1|I) d\theta'_0 d\theta'_1
    \]</div>
<ul class="simple">
<li><p>We can relate the primed and unprimed <span class="math notranslate nohighlight">\(\theta\)</span>’s by substituting:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    y = \theta_1 x + \theta_0 = \theta_1(\theta'_1 y + \theta'_0) + \theta_0
    = \theta_1 \theta'_1 y + \theta_1\theta'_0 + \theta_0
    \]</div>
<div class="math notranslate nohighlight">
\[
   \Lra \theta_1 \theta'_1 =1, \quad \theta_1\theta'_0+\theta_0 = 0
   \quad\Lra\quad \theta_1' = \theta_1^{-1}, \quad \theta'_0 = -\theta_1^{-1}\theta_0
    \]</div>
<ul class="simple">
<li><p>This lets us calculate the Jacobian:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
     \left| \det\pmatrix{ 
     \frac{\partial\theta_0}{\partial\theta'_{0}} &amp;
     \frac{\partial\theta_0}{\partial\theta'_{1}} \\
     \frac{\partial\theta_1}{\partial\theta'_{0}} &amp;
     \frac{\partial\theta_1}{\partial\theta'_{1}}}
     \right|
     =
     \left| 
     \begin{array}{cc}
       -\theta'_1{}^{-1} &amp; \theta'_0 \\ 
     0 &amp; -\theta'_1{}^{-2}
     \end{array} 
     \right|
     = \frac{1}{\theta'_1{}^3} = \theta_1^3 .
    \end{split}\]</div>
<ul class="simple">
<li><p>That means that</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
     p(\theta_0,\theta_1|I) d\theta_0 d\theta_1
     &amp; = p(-\theta_1^{-1}\theta_0,\theta_1^{-1}|I)  d\theta_0 d\theta_1 \frac{1}{\theta_1^3} \\
     \mbox{or}\ \theta_1^3 p(\theta_0,\theta_1|I)  &amp;= p(-\theta_1^{-1}\theta_0,\theta_1^{-1}|I).
    \end{align}\end{split}\]</div>
<ul class="simple">
<li><p>One possible solution is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
      p(\theta_0,\theta_1|I) \propto (1+\theta_1^2)^{-3/2} .
    \]</div>
</li>
</ul>
<section id="principle-of-maximum-entropy">
<h3>Principle of Maximum Entropy<a class="headerlink" href="#principle-of-maximum-entropy" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Arguing from monkeys distributing <span class="math notranslate nohighlight">\(N\)</span> balls into <span class="math notranslate nohighlight">\(M\)</span> boxes, so <span class="math notranslate nohighlight">\(n_i\)</span> in each box and <span class="math notranslate nohighlight">\(N = \sum_{i=1}^M n_i\)</span>.</p></li>
<li><p>We’ll let them do this many times, subject to the constraints described by <span class="math notranslate nohighlight">\(I\)</span>.</p>
<ul class="simple">
<li><p>The idea is to find the pdf specified by <span class="math notranslate nohighlight">\(p_i = n_i/N\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> that appears most often <span class="math notranslate nohighlight">\(\Lra\)</span> this best represents our state of knowledge.</p></li>
</ul>
</li>
<li><p>So this becomes a matter of counting microstates (i.e., a particular distribution <span class="math notranslate nohighlight">\(\{n_i\}\)</span>) that are most likely given the constraints.</p>
<ul>
<li><p>We’ll let <span class="math notranslate nohighlight">\(F(\{p_i\}) = \text{# of ways to get } \{n_i\} / \text{total # of ways} = M^N\)</span>.</p></li>
<li><p>Now do some combinatorics <span class="math notranslate nohighlight">\(\Lra\)</span> this is a multinomial distribution (we use Stirling here: <span class="math notranslate nohighlight">\(n! \approx n\log n - n\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
          \log F(\{p_i\}) &amp;= \log(N!) - \sum_{i=1}^M \log(n_i!) -     N\log M \\
          &amp; \approx -N\log M + N\log N - \sum_{i=1}^M n_i\log(n_i) \\
          &amp; \approx -N\log M - N \sum_{i=1}^M p_i\log(p_i)
        \end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_i = n_i/N\)</span>.</p>
</li>
</ul>
</li>
<li><p>This tells us that the key piece to maximize is the entropy:</p>
<div class="math notranslate nohighlight">
\[ S = - \sum_{i=1}^M p_i \log(p_i) . \]</div>
</li>
<li><p>There are several arguments for maximizing the entropy:</p>
<ol class="arabic simple">
<li><p>information theory, which says says maximum entropy equals minimum information (Shannon, 1948);</p></li>
<li><p>logical consistency (Shore and Johnson, 1960);</p></li>
<li><p>uncorrelated assignments are related monotonically to <span class="math notranslate nohighlight">\(S\)</span> (Skilling, 1988).</p></li>
</ol>
</li>
<li><p>The third one tells us that unless you know specifically about correlations, it should not be in your probability assignment. One finds that entropy maximization satisfies this condition (see the notebook for a comparison of different possibilities for a test problem).</p></li>
<li><p>The continuous version of entropy is</p>
<div class="math notranslate nohighlight">
\[
       S[p(x)] = - \int dx p(x) \log\Bigl(\frac{p(x)}{m(x)}\Bigr)
    \]</div>
<p>where <span class="math notranslate nohighlight">\(m(x)\)</span> is a measure function. It is there to ensure that <span class="math notranslate nohighlight">\(S[p]\)</span> is invariant under <span class="math notranslate nohighlight">\(x\rightarrow y=f(x)\)</span>. Typically this means <span class="math notranslate nohighlight">\(m(x) = \)</span> constant.</p>
</li>
</ul>
</section>
<section id="example-1-gaussian-distribution">
<h3>Example 1: Gaussian distribution<a class="headerlink" href="#example-1-gaussian-distribution" title="Permalink to this heading">#</a></h3>
<p>The constraints are normalization and a known variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
   \int_{0}^\infty dx\, p(x) &amp;= 1, \quad x\geq 0 \\
   \int_{0}^\infty dx\, (x-\mu)^2 p(x) &amp;= \sigma^2
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\Lra\)</span> maximize</p>
<div class="math notranslate nohighlight">
\[
  Q(p;\lambda_0,\lambda_1) = - \int dx\, p(x)\log\Bigl(\frac{p(x)}{m(x)}\Bigr) + \lambda_0 \bigl(1 - \int dx\, p(x)\bigr)
  + \lambda_1 \bigl(\sigma^2 - \int dx\, (x-\mu)^2 p(x)\bigr) ,
\]</div>
<p>with uniform <span class="math notranslate nohighlight">\(m(x)\)</span> (we take <span class="math notranslate nohighlight">\(m(x) = 1\)</span> below). The maximization is straightforward.</p>
<p><strong>Step 1:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \frac{\delta Q}{\delta p(x)}
 &amp;= -\log\frac{p(x)}{1} - \frac{p(x)}{p(x)} - \lambda_0 - \lambda_1 (x-\mu)^2 \\
  \frac{\delta Q}{\delta \lambda_0} &amp;= 1 - \int_{-\infty}^{\infty} dx\, p(x) \quad\text{and}\quad
 \frac{\delta Q}{\delta \lambda_1} = \sigma^2 - \int_{-\infty}^{\infty} dx\, (x-\mu)^2 p(x) .
\end{align}\end{split}\]</div>
<p><strong>Step 2:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 &amp; \frac{\delta Q}{\delta p(x)} = 0 \Lra \log p(x) = -(1 + \lambda_0) - \lambda_1 (x-\mu)^2 \\
 &amp; \Lra p(x) = e^{-1+\lambda_0}e^{-\lambda_1 (x-\mu)^2} .
\end{align}\end{split}\]</div>
<p><strong>Step 3:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 &amp; \frac{\partial Q}{\partial \lambda_0} = 0 \Lra \int_{-\infty}^{\infty} dx\, e^{-1+\lambda_0}e^{-\lambda_1 (x-\mu)^2} =
 e^{-1+\lambda_0} \sqrt{\frac{\pi}{\lambda_1}} = 1
 \Lra e^{-1+\lambda_0} = \sqrt{\frac{\lambda_1}{\pi}}   \\
 &amp; \frac{\partial Q}{\partial \lambda_1} = 0 \Lra \int_{-\infty}^{\infty} dx\, (x-\mu)^2 e^{-1+\lambda_0}e^{-\lambda_1 (x-\mu)^2} = \underbrace{e^{-1+\lambda_0}\frac{1}{\lambda_1^{3/2}}}_{1/\sqrt{\pi}\lambda_1} \underbrace{\int_{-\infty}^{\infty} dy\, y^2 e^{-y^2}}_{\sqrt{\pi}/2} = \sigma^2
 \Lra \lambda_1 = \frac{1}{2\sigma^2}
\end{align}\end{split}\]</div>
<p>Putting it together we get our good friend the Gaussian distribution:</p>
<div class="math notranslate nohighlight">
\[
  p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}  e^{-(x-\mu)^2/2\sigma^2} .
\]</div>
</section>
<section id="example-2-poisson-distributions">
<h3>Example 2: Poisson distributions<a class="headerlink" href="#example-2-poisson-distributions" title="Permalink to this heading">#</a></h3>
<p>The constraints are normalization and a known mean:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
   \int_{0}^\infty dx\, p(x) &amp;= 1, \quad x\geq 0 \\
   \int_{0}^\infty dx\, x p(x) &amp;= \mu
\end{align}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\Lra\)</span> maximize</p>
<div class="math notranslate nohighlight">
\[
  Q(p;\lambda_0,\lambda_1) = - \int dx\, p(x)\log\Bigl(\frac{p(x)}{m(x)}\Bigr) + \lambda_0 \bigl(1 - \int dx\, p(x)\bigr)
  + \lambda_1 \bigl(\mu - \int dx\, x p(x)\bigr) ,
\]</div>
<p>with uniform <span class="math notranslate nohighlight">\(m(x)\)</span>. The maximization is again straightforward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
 &amp; \frac{\delta Q}{\delta p(x)}
 = -\log\frac{p(x)}{1} - \frac{p(x)}{p(x)} - \lambda_0 - \lambda_1 x = 0 \\
 &amp; \Lra \log p(x) = -(1 + \lambda_0) - \lambda_1 x \\
 &amp; \Lra p(x) = e^{-1+\lambda_0}e^{-\lambda_1 x}
\end{align}\end{split}\]</div>
<p>Finally, we determine <span class="math notranslate nohighlight">\(\lambda_0\)</span> and <span class="math notranslate nohighlight">\(\lambda_1\)</span> from the constraints:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  e^{1+\lambda_0}\int_0^\infty dx\, e^{-\lambda_1 x}
    = e^{-(1+\lambda_0)}\frac{1}{\lambda_1} = 1
    \quad&amp;\Lra\quad \lambda_1 = e^{-(1+\lambda_0)} \\
   \int_0^\infty dx\, \underbrace{e^{-(1+\lambda_0)}}_{\lambda_1}
 \underbrace{e^{-\lambda_1 x}x}_{1/\lambda_1^2}
  = \mu
  \quad&amp;\Lra\quad \lambda_1 = \frac{1}{\mu} .
\end{align}\end{split}\]</div>
<p>Substituting we get the Poisson distribution:</p>
<div class="math notranslate nohighlight">
\[
  p(x) = \frac{1}{\mu}  e^{-x/\mu} .
\]</div>
<p>Try the other examples!</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "DanielRPhillips/LearningFromData",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/Maximum_entropy"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="max_ent.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>Maximum entropy</p>
      </div>
    </a>
    <a class="right-next"
       href="../../notebooks/Maximum_entropy/MaxEnt.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.2. </span>Ignorance pdfs: Indifference and translation groups</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#follow-up-to-gaussian-process-exercises">Follow-up to Gaussian process exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-entropy">Maximum entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principle-of-maximum-entropy">Principle of Maximum Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-gaussian-distribution">Example 1: Gaussian distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-poisson-distributions">Example 2: Poisson distributions</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dick Furnstahl
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>