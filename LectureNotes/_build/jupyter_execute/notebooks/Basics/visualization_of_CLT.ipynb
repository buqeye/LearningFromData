{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the Central Limit Theorem\n",
    "\n",
    " The general statement of the central limit theory (CLT) is that the sum of $n$ random variables drawn from *any* pdf (or multiple pdfs) of finite variance $\\sigma^2$ tends as $n\\rightarrow\\infty$ to be Gaussian distributed about the expectation value of the sum, with variance $n\\sigma^2$.  (So we would scale the sum by $1/\\sqrt{n}$ to keep the variance fixed.) In this notebook we look visually at several consequences of the CLT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for plots in this notebook using matplotlib \n",
    "%matplotlib inline   \n",
    "\n",
    "import scipy.stats as stats  # We'll use stats as our source of pdfs\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # nicer plots!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit of Poisson distribution is Gaussian\n",
    "\n",
    "One consequence of the CLT is that distributions such as the Binomial and Poisson distributions all tend to look like Gaussian distributions in the limit of a large number of drawings.\n",
    "Here we visualize how the Poisson distribution in the limit $D \\rightarrow \\infty$ approaches a Gaussian distribution:\n",
    "\n",
    "$$\n",
    " p(N \\mid D) = \\frac{D^N e^{-D}}{N!} \n",
    " \\stackrel{D\\rightarrow\\infty}{\\longrightarrow} \\frac{1}{\\sqrt{2\\pi D}}e^{-(N-D)^2/(2D)}\n",
    "$$\n",
    "\n",
    "with $N$ an integer. *You will be asked to prove this limit in a later notebook.*\n",
    "\n",
    "**Note: this limiting functional form is not obviously connected to the general statement of the CLT up above. What is the connection of the Gaussian limit to a sum of random variables as prescribed by the CLT?** The answer is that a sum of Poisson distributed random variables is itself Poisson distributed. For example, if $X_1,\\ldots,X_D$  are independent Poisson random variables with mean 1 ($D$ is an integer here), then $Y = \\sum_{i=1}^D X_i$ is a Poisson random variable with mean $D$. So the sum is built in by directly considering $Y$, as we do below. Note the analogy to the situation with the binomial distribution and the CLT discussed in Lecture 4.\n",
    "\n",
    "\n",
    "This visualization is adapted from the `amplitude_in_presence_of_background_RECAP.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson(N, D):\n",
    "    \"\"\"\n",
    "    Returns a Poisson distribution value with mean D for integer N.\n",
    "    We require N to be an integer greater than equal to zero.\n",
    "    \"\"\"\n",
    "    assert (isinstance(N, int) and N >= 0), \\\n",
    "            \"N must be a non-negative integer!\"\n",
    "\n",
    "    return D**N * np.exp(-D) / factorial(N) \n",
    "\n",
    "def poisson_plot(ax, D, max_N):\n",
    "    \"\"\"\n",
    "    Make a bar plot on the axis ax of the Poisson distribution for mu = D\n",
    "     and out to a maximum integer max_N.\n",
    "    UPDATE: we eplaced our own poisson function with poisson.pmf from \n",
    "     scipy.stats so that we can go to larger N. \n",
    "    \"\"\"\n",
    "    N_pts = np.arange(0, max_N, 1, dtype=int)\n",
    "    poisson_pts = [stats.poisson.pmf(N, D) for N in N_pts] # poisson_pts = [poisson(int(N), D) for N in N_pts]\n",
    "    \n",
    "    ax.bar(N_pts, poisson_pts, width=0.8, bottom=None, align='center')\n",
    "    ax.set_xlabel(r'Number of counts $N$', fontsize=18)\n",
    "    ax.set_ylabel(fr'$\\mathrm{{p}}(N \\mid D={D:.1f})$', fontsize=18)\n",
    "    ax.set_title(rf'$D = {D:.1f}$', fontsize=20)\n",
    "    ax.tick_params(labelsize=16)\n",
    "    return N_pts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make successive histogram plots of the Poisson distribution, increasing the value of $D$ from 0.5 to 50, and superposing the normal distribution it is supposed to approach.\n",
    "\n",
    "Note that the arguments of stats.norm.pdf are the # of points, the mean, and the standard deviation (not the variance) of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_list = np.array([0.5, 1.2, 1.7, 12.5, 20., 50., 100., 150.])\n",
    "num_rows = int(D_list.size / 2)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(15, 5 * num_rows))\n",
    "\n",
    "for D, ax in zip(D_list, axes.flatten()):\n",
    "    max_N = int(D + 6.*np.sqrt(D))  # mean + multiple of standard deviation\n",
    "    N_pts = np.arange(0, max_N, 0.01)\n",
    "    y_pts = stats.norm.pdf(N_pts, D, np.sqrt(D))\n",
    "    poisson_plot(ax, D, max_N)\n",
    "    ax.plot(N_pts, y_pts, color='red')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior of the mean of a fixed-size sample \n",
    "\n",
    "Here we take the mean of a fixed-size sample drawn from any pdf, and then repeat a number of times and look at the distribution of the means.  According to the CLT, this distribution should approach a Gaussian pdf.  For our test pdf we'll use a uniform distribution, although this is easy to switch to another distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Sample the distribution we will use (by default a uniform distribution)   \n",
    "total_draws = 100000\n",
    "uni_min, uni_max = 0., 1.\n",
    "ran_uniform_array = np.random.uniform(uni_min, uni_max, total_draws)\n",
    "\n",
    "bins = 15  # bins for the historam\n",
    "\n",
    "# Find the mean and standard deviation of our distribution, to use for \n",
    "#  plotting a comparison Gaussian distribution.\n",
    "mu = ran_uniform_array.mean()\n",
    "sigma = ran_uniform_array.std()\n",
    "\n",
    "print(f' mean of uniform array = {mu:.3f}')\n",
    "print(f' standard deviation of uniform array = {sigma:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_ax(ax, means_array, N_means, sample_size, bins, CLT_pdf=False):\n",
    "    \"\"\"\n",
    "    Plot a histogram on axis ax that shows the means.  Add the expected\n",
    "     limiting normal distribution if CLT_pdf is True.\n",
    "    \"\"\"\n",
    "    sigma_tilde = sigma / np.sqrt(sample_size)\n",
    "    x_min = mu - 4.*sigma_tilde\n",
    "    x_max = mu + 4.*sigma_tilde\n",
    "    \n",
    "    (_, bin_array, _) = ax.hist(means_array[:N_means], bins = bins, \n",
    "                                    align='mid')\n",
    "    bin_width = bin_array[1] - bin_array[0]\n",
    "    title_string = fr'Sampling size n={sample_size} of uniform pdf ' + \\\n",
    "                   fr'drawn {N_means:d} times'\n",
    "    ax.set_title(title_string)\n",
    "    ax.set_xlabel('Value of Mean')\n",
    "    ax.set_ylabel('Count in bin')\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "\n",
    "    if (CLT_pdf):  # if true, plot a normal pdf with the same mu and sigma\n",
    "                   #  divided by the sqrt of the sample size.\n",
    "        x_pts = np.linspace(x_min, x_max, 200)\n",
    "        y_pts = stats.norm.pdf(x_pts, mu, sigma_tilde)\n",
    "        ax.plot(x_pts, y_pts * (bin_width*N_means), color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_result(sample_size, bins):\n",
    "    \"\"\"Plot a series of histograms to show the approach to a Gaussian.\"\"\"\n",
    "    sample_means_fixed_sample_size = []\n",
    "    for i in range(10000):\n",
    "        samples = ran_uniform_array[np.random.randint(ran_uniform_array.size, \n",
    "                                                      size = sample_size)]\n",
    "        sample_means_fixed_sample_size.append(np.mean(samples))\n",
    "    \n",
    "    N_means_array = np.array([1, 5, 10, 20, 50, 100, 500, 1000, 10000])\n",
    "    \n",
    "    fig3 = plt.figure(figsize=(15,10))\n",
    "    \n",
    "    axes = fig3.subplots(nrows=3, ncols=3)\n",
    "    fig3.suptitle(f'Sample size n={sample_size:d} sampled by various times', \n",
    "                  fontsize=16, va='baseline')\n",
    "     \n",
    "    for index, ax in enumerate(axes.flatten()):\n",
    "        histogram_ax(ax, sample_means_fixed_sample_size, N_means_array[index], \n",
    "                     sample_size, bins, CLT_pdf=True)\n",
    "         \n",
    "    fig3.tight_layout(w_pad=1.8)\n",
    "    fig3.subplots_adjust(top = 0.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First example: each sample is only one point\n",
    "\n",
    "Since the sample size is one, the mean is just the point, so the distributions here will look like the original distribution (no approach to Gaussian). Note that we bin the trivial means (the individual draws) into 20 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_result(1, 20)   # n=1 so just the original distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we just get a better and better reproduction of the original distribution, which is uniform in this case. We don't expect any connection to a Gaussian from the CLT in this case. That is, just taking a lot of samples from a distribution doesn't mean in general that we will get a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second example: each sample is two points\n",
    "\n",
    "So the mean is the average of the two points. We divide the means into 20 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_result(2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of means gets larger, we see that a peak develops at 1/2 and the shape becomes triangular. If we think of the average of two uniformly drawn points from \\[0,1\\], it is most likely that their average will be close to 1/2 and unlikely to be at the ends (e.g., to get near 0, one would need both numbers to be close to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third example: each sample is 10 points\n",
    "\n",
    "Now we see the trends from $n=2$ being accentuated. By the time we have 10000 means to histogram, it looks like a pretty good Gaussian. **Note that the width of the Gaussian is set by the sample size $n=10$, not by how many draws are made.** The width decreases as $1/\\sqrt{n}$, so from $n=2$ above to $n=10$ here it is roughly a factor of 2.2 smaller (0.25 vs. 0.1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_result(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth example: each sample is 50 points\n",
    "\n",
    "By now we are taking the mean of 50 points with each sample. This will increasingly be close to 1/2 and very rarely far from 1/2. The width decreases as $1/\\sqrt{n}$, so from $n=10$ to $n=50$ it is roughly a factor of 2.2 smaller (0.1 vs. 0.04)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_result(50, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding $n$ variables drawn from a distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the results derived for combining $n$ variables drawn from the same zero-mean probability distribution with an overall $1/\\sqrt{n}$.  The generalization to different pdfs (and to non-zero means) is straightforward but should be coded more efficiently.\n",
    "\n",
    "$$\n",
    "  p(z)  = \\int_{-\\infty}^{+\\infty} \\frac{d\\omega}{2\\pi}\\, e^{-i\\omega z}\n",
    "    \\left[ \\int_{-\\infty}^{+\\infty}\\! dx_1\\, e^{i\\omega x_1/\\sqrt{n}} p(x_1) \\right]\n",
    "    \\left[ \\int_{-\\infty}^{+\\infty}\\! dx_2\\, e^{i\\omega x_2/\\sqrt{n}} p(x_2) \\right]\n",
    "    \\cdots\n",
    "    \\left[ \\int_{-\\infty}^{+\\infty}\\! dx_n\\, e^{i\\omega x_n/\\sqrt{n}} p(x_n) \\right]\n",
    "$$\n",
    "\n",
    "So the idea will be to take a distribution, do an FT with the frequency divided by $\\sqrt{n}$, raise it to the $n^{\\rm th}$ power, then do an inverse FT.  This should approach a Gaussian as $n$ gets large based on the proof we worked through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Fourier transforms by numerical integration (rather than FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try first with the `scipy.integrate` function quad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def cosineFT(omega, fun):\n",
    "    \"\"\"Calculate the cosine Fourier transform (CFT) using quad from\n",
    "       scipy.integrate with the cosine weight. \n",
    "    \"\"\"\n",
    "    ans, error = quad(fun, 0., np.inf, weight='cos', wvar=omega)\n",
    "    return 2.*ans, error   # note the 2 because integration from [0, inf]\n",
    "    \n",
    "def cosineFT2(omega, fun):\n",
    "    \"\"\"Calculate the cosine Fourier transform (CFT) using quad from\n",
    "       scipy.integrate with the cosine term added explicitly. \n",
    "    \"\"\"\n",
    "    integrand = lambda x: np.cos(omega * x) * fun(x)\n",
    "    ans, error = quad(integrand, -np.inf, np.inf)\n",
    "    return ans, error\n",
    "\n",
    "def invCFT(t, omega_pts, FT_pts):\n",
    "    \"\"\"Calculate the inverse cosine Fourier transform (invCFT) using numpy's\n",
    "       trapz function. Includes 1/2\\pi factor \n",
    "    \"\"\"\n",
    "    integrand_pts = np.array( [np.cos(omega * t) * FT_value \\\n",
    "                               for omega, FT_value in zip(omega_pts, FT_pts)] \n",
    "                            )\n",
    "    return np.trapz(integrand_pts, omega_pts) / (2. * np.pi)\n",
    "\n",
    "def gaussian(x, sigma=1.):\n",
    "    \"\"\"One-dimensional normalized Gaussian.\"\"\"\n",
    "    return 1./np.sqrt(2. * np.pi * sigma**2) * np.exp(-x**2/(2.*sigma**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a uniform distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT of uniform distribution\n",
    "\n",
    "x_pts = np.linspace(-4., 4., 401)  # x range\n",
    "\n",
    "# uniform distribution from -1 to +1\n",
    "uni_dist = stats.uniform(loc=-1., scale=2.)\n",
    "uni_dist_pts = uni_dist.pdf(x_pts)\n",
    "\n",
    "uni_gauss_pts = np.array([gaussian(x, sigma=uni_dist.std()) for x in x_pts])\n",
    "\n",
    "omega_pts = np.linspace(-40., 40., 401)\n",
    "#FT_uniform = np.array([cosineFT(omega, uni_dist.pdf)[0] for omega in omega_pts])\n",
    "\n",
    "def CFT_n(fun, omega_pts, n=1):\n",
    "    \"\"\"Take the Fourier transform of fun wrt omega/\\sqrt{n} and then\n",
    "        raise it to the n'th power.\n",
    "    \"\"\"\n",
    "    CFT_n_pts = np.array([cosineFT(omega / np.sqrt(n), fun)[0] \\\n",
    "                          for omega in omega_pts])\n",
    "    return CFT_n_pts**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_vals = np.array([1, 2, 3, 4, 5, 8])\n",
    "FT_uniform_pts = np.array([CFT_n(uni_dist.pdf, omega_pts, n) \\\n",
    "                           for n in n_vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invFT_uniform_pts = np.array([[invCFT(x, omega_pts, FT_uniform_pts[i]) \n",
    "                              for x in x_pts] for i in range(n_vals.size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below on the right are the $n$ products of the Fourier-transformed distributions\n",
    "(so this is in the Fourier space) with the frequency divided by $\\sqrt{n}$. \n",
    "The plots on the left are the inverse Fourier transforms of the plots on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(n_vals.size, 2, figsize=(15, 5 * n_vals.size))\n",
    "ax[0, 0].plot(x_pts, uni_dist_pts, color='blue')\n",
    "\n",
    "for index, n in enumerate(n_vals):\n",
    "    ax[index, 0].plot(x_pts, invFT_uniform_pts[index],\n",
    "                      color='blue', label=rf'invFT[uniform] $n={n}$')\n",
    "    ax[index, 0].plot(x_pts, uni_gauss_pts, color='red',\n",
    "                      label='CLT gaussian')\n",
    "    ax[index, 0].legend()\n",
    "    ax[index, 0].set_title(rf'$n = {n}$')\n",
    "    ax[index, 1].plot(omega_pts, FT_uniform_pts[index],\n",
    "                      color='blue', label=rf'FT[uniform] $n={n}$')\n",
    "    ax[index, 1].legend()\n",
    "    ax[index, 1].set_title(rf'$n = {n}$')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that multiplying the original Fourier transform will rapidly kill off the tails and highlight the central part. The $n$ scalings enforces that the Gaussians in both the original and Fourier space have constant variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}