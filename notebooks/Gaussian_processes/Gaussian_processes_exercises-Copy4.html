
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Physics 8805 &#8212; Learning from data</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script>window.MathJax = {"tex": {"macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "alphavec": ["\\boldsymbol{\\alpha}"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "fvec": ["\\boldsymbol{f}"], "mvec": ["\\boldsymbol{m}"], "qvec": ["\\boldsymbol{q}"], "rvec": ["\\boldsymbol{r}"], "uvec": ["\\boldsymbol{u}"], "wvec": ["\\boldsymbol{w}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"], "gs": ["{0}"]}}}</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/8820_icon.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning from data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../about.html">
   About this Jupyter Book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Course overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Course/overview.html">
   Objectives
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Basics/basics.html">
   1. Basics of Bayesian statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_01.html">
     1.1. Lecture 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Exploring_pdfs.html">
     1.2. Exploring PDFs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/simple_sum_product_rule.html">
     1.3. Checking the sum and product rules, and their consequences
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_02.html">
     1.4. Lecture 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/Bayesian_updating_coinflip_interactive.html">
     1.5. Interactive Bayesian updating: coin flipping example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/medical_example_by_Bayes.html">
     1.6. Standard medical example by applying Bayesian rules of probability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/radioactive_lighthouse_exercise.html">
     1.7. Radioactive lighthouse problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Basics/lecture_03.html">
     1.8. Lecture 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Parameter_estimation/param_est.html">
   2. Bayesian parameter estimation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_04.html">
     2.1. Lecture 4: Parameter estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_Gaussian_noise.html">
     2.2. Parameter estimation example: Gaussian noise and averages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/Assignment_extending_radioactive_lighthouse.html">
     2.3. Assignment: 2D radioactive lighthouse location using MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_05.html">
     2.4. Lecture 5
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">
     2.5. Parameter estimation example: fitting a straight line
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/demo-ModelValidation.html">
     2.6. Linear Regression and Model Validation demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Parameter_estimation/lecture_06.html">
     2.7. Lecture 6
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/amplitude_in_presence_of_background.html">
     2.8. Amplitude of a signal in the presence of background
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/Assignment_parameter_estimation_followups.html">
     2.9. Assignment: Follow-ups to Parameter Estimation notebooks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/exercise_LinearRegression.html">
     2.10. Linear Regression exercise
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/linear_algebra_games_I.html">
     2.11. Linear algebra games including SVD for PCA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">
     2.12. Follow-up: fluctuation trends with # of points and data errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/MCMC_sampling_I/MCMC_sampling_I.html">
   3. MCMC sampling I
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_I/lecture_07.html">
     3.1. Lecture 7
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/Metropolis_Poisson_example.html">
     3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_I/lecture_08.html">
     3.3. Lecture 8
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/MCMC-random-walk-and-sampling.html">
     3.4. Exercise: Random walk
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Why_Bayes_is_better/bayes_is_better.html">
   4. Why Bayes is better
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_09.html">
     4.1. Lecture 9
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/bayes_billiard.html">
     4.2. A Bayesian Billiard game
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_10.html">
     4.3. Lecture 10
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">
     4.4. Parameter estimation example: fitting a straight line II
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_11.html">
     4.5. Lecture 11
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">
     4.6. Error propagation: Example 3.6.2 in Sivia
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/visualization_of_CLT.html">
     4.7. Visualization of the Central Limit Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Basics/correlation_intuition.html">
     4.8. Building intuition about correlations (and a bit of Python linear algebra)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_12.html">
     4.9. Lecture 12
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_I/MCMC-diagnostics.html">
     4.10. Overview: MCMC Diagnostics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Why_Bayes_is_better/lecture_13.html">
     4.12. Lecture 13
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Why_Bayes_is_better/dealing_with_outliers.html">
     4.13. Dealing with outliers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Model_selection/model_selection.html">
   5. Model selection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_14.html">
     5.1. Lecture 14
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_15.html">
     5.2. Lecture 15
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Model_selection/Evidence_for_model_EFT_coefficients.html">
     5.3. Evidence calculation for EFT expansions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Model_selection/lecture_16.html">
     5.4. Lecture 16
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mini-projects/MCMC-parallel-tempering_ptemcee.html">
     5.5. Example: Parallel tempering for multimodal distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mini-projects/MCMC-parallel-tempering_ptemcee_vs_zeus.html">
     5.6. Example: Parallel tempering for multimodal distributions vs. zeus
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/MCMC_sampling_II/MCMC_sampling_II.html">
   6. MCMC sampling II
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_II/lecture_17.html">
     6.1. Lecture 17
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/chi_squared_tests.html">
     6.2. Quick check of the distribution of normal variables squared
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/Liouville_theorem_visualization.html">
     6.3. Liouville Theorem Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">
     6.4. Solving orbital equations with different algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/MCMC_sampling_II/lecture_18.html">
     6.5. Lecture 18
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/PyMC3_intro_updated.html">
     6.6. PyMC3 Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MCMC_sampling_II/PyMC3_docs_getting_started_updated.html">
     6.7. Getting started with PyMC3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Parameter_estimation/parameter_estimation_Gaussian_noise_compare_samplers.html">
     6.8. Comparing samplers for a simple problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mini-projects/zeus_multimodal.html">
     6.9. zeus: Sampling from multimodal distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Gaussian_processes/gaussian_processes.html">
   7. Gaussian processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Gaussian_processes/lecture_19.html">
     7.1. Lecture 19
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="demo-GaussianProcesses.html">
     7.2. Gaussian processes demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="GaussianProcesses.html">
     7.3. Learning from data: Gaussian processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Gaussian_processes_exercises.html">
     7.4. Exercise: Gaussian Process models with GPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Gaussian_processes/lecture_20.html">
     7.5. Lecture 20
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Maximum_entropy/max_ent.html">
   8. Assigning probabilities
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Maximum_entropy/lecture_21.html">
     8.1. Lecture 21
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/MaxEnt.html">
     8.2. Ignorance pdfs: Indifference and translation groups
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/Pdfs_from_MaxEnt.html">
     8.3. MaxEnt for deriving some probability distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/MaxEnt_Function_Reconstruction.html">
     8.4. Maximum Entropy for reconstructing a function from its moments
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Maximum_entropy/demo-MaxEnt.html">
     8.5. Making figures for Ignorance PDF notebook
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Machine_learning/machine_learning.html">
   9. Machine learning: Bayesian methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Machine_learning/lecture_22.html">
     9.1. Lecture 22
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Bayesian_optimization.html">
     9.2. Bayesian Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Machine_learning/lecture_23.html">
     9.3. Lecture 23
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Neural_networks_explained.html">
     9.4. What Are Neural Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Forssen_tif285_NeuralNet.html">
     9.5. Neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Forssen_tif285_demo-NeuralNet.html">
     9.6. Neural network classifier demonstration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Bayesian_neural_networks_tif285.html">
     9.7. Bayesian neural networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/Machine_learning/lecture_24.html">
     9.8. Lecture 24
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/demo-Bayesian_neural_networks_tif285.html">
     9.9. Variational Inference: Bayesian Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine_learning/Convolutional_neural_network_explained.html">
     9.10. What is a convolutional neural network?
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/SVD/svd.html">
   10. PCA, SVD, and all that
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../content/SVD/lecture_25.html">
     10.1. Lecture 25
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../SVD/linear_algebra_games_including_SVD.html">
     10.2. Linear algebra games including SVD for PCA
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Mini-projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_I_toy_model_of_EFT.html">
   Mini-project I: Parameter estimation for a toy model of an EFT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/model-selection_mini-project-IIa.html">
   Mini-project IIa: Model selection basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">
   Mini-project IIb: How many lines are there?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_IIIa_bayesian_optimization.html">
   Mini-project IIIa: Bayesian optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">
   Mini-project IIIb: Bayesian Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Reference material
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/zbibliography.html">
   Bibliography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/related_topics.html">
   Related topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Reference/installing_anaconda.html">
   Using Anaconda
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/Reference/using_github.html">
   Using GitHub
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../content/Reference/python_jupyter.html">
   Python and Jupyter notebooks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Reference/Jupyter_Python_intro_01.html">
     Python and Jupyter notebooks: part 01
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Reference/Jupyter_Python_intro_02.html">
     Python and Jupyter notebooks: part 02
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../content/jb_tests.html">
   Examples: Jupyter jb-book
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Notebook keys
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/simple_sum_product_rule_KEY.html">
   Checking the sum and product rules, and their consequences
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/medical_example_by_Bayes_KEY.html">
   Standard medical example by applying Bayesian rules of probability
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Basics/radioactive_lighthouse_exercise_key.html">
   Radioactive lighthouse problem
   <span style="color: red">
    Key
   </span>
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/Gaussian_processes/Gaussian_processes_exercises-Copy4.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/furnstahl/Physics-8820"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/furnstahl/Physics-8820/issues/new?title=Issue%20on%20page%20%2Fnotebooks/Gaussian_processes/Gaussian_processes_exercises-Copy4.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/furnstahl/Physics-8820/main?urlpath=tree/./LectureNotes/notebooks/Gaussian_processes/Gaussian_processes_exercises-Copy4.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-from-data-bayesian-methods-and-machine-learning">
   Learning from Data: Bayesian Methods and Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autumn-2019">
     Autumn, 2019
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-gaussian-process-models-with-gpy">
   Exercise: Gaussian Process models with GPy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-modules">
     Import modules
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-started-the-covariance-function">
   1 Getting started: The Covariance Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setting-covariance-function-parameters">
   Setting Covariance Function Parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     Exercise 1
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#covariance-functions-in-gpy">
   Covariance Functions in GPy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-the-covariance-function-given-the-input-data-mathbf-x">
   Computing the Covariance Function given the Input Data,
   <span class="math notranslate nohighlight">
    \(\mathbf{X}\)
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#follow-up-task">
     Follow-up task
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combining-covariance-functions">
   Combining Covariance Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2">
     Exercise 2
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-covariance-functions-in-gpy">
     Combining Covariance Functions in GPy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-from-a-gaussian-process">
   2 Sampling from a Gaussian Process
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3">
     Exercise 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-gaussian-process-regression-model">
   3 A Gaussian Process Regression Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-4">
     Exercise 4
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance-function-parameter-estimation">
     Covariance Function Parameter Estimation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-running-example">
   4 A Running Example
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-5">
     Exercise 5
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="physics-8805">
<h1>Physics 8805<a class="headerlink" href="#physics-8805" title="Permalink to this headline">¶</a></h1>
<div class="section" id="learning-from-data-bayesian-methods-and-machine-learning">
<h2>Learning from Data: Bayesian Methods and Machine Learning<a class="headerlink" href="#learning-from-data-bayesian-methods-and-machine-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="autumn-2019">
<h3>Autumn, 2019<a class="headerlink" href="#autumn-2019" title="Permalink to this headline">¶</a></h3>
<p><span class="math notranslate nohighlight">\(% Some LaTeX definitions we'll use.
\newcommand{\pr}{\textrm{p}}
\)</span></p>
</div>
</div>
<div class="section" id="exercise-gaussian-process-models-with-gpy">
<h2>Exercise: Gaussian Process models with GPy<a class="headerlink" href="#exercise-gaussian-process-models-with-gpy" title="Permalink to this headline">¶</a></h2>
<p>Adapted from Christian Forssen, TALENT Course 11, June, 2019, which was
adapted from the Gaussian Process Summer School (written by Nicolas Durrande, Neil Lawrence and James Hensman).</p>
<p>Additions by Dick Furnstahl in November, 2019.</p>
<p>The aim of this exercise is to illustrate the concepts of Gaussian processes. We will focus on three aspects of GPs: the kernel, the random sample paths and the GP regression model.</p>
<p>We will use the well known <a class="reference external" href="https://sheffieldml.github.io/GPy/">GPy package</a> by the Sheffield ML group.</p>
<p>The current draft of the online documentation of GPy is available from <a class="reference external" href="http://gpy.readthedocs.org/en/latest/">this page</a>.</p>
<div class="section" id="import-modules">
<h3>Import modules<a class="headerlink" href="#import-modules" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">GPy</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="getting-started-the-covariance-function">
<h2>1 Getting started: The Covariance Function<a class="headerlink" href="#getting-started-the-covariance-function" title="Permalink to this headline">¶</a></h2>
<p>Let’s start with defining an exponentiated quadratic covariance function (also known as squared exponential or rbf or Gaussian) in one dimension:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="mi">1</span>          <span class="c1"># input dimension</span>
<span class="n">var</span> <span class="o">=</span> <span class="mf">1.</span>       <span class="c1"># variance</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.2</span>    <span class="c1"># lengthscale</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="n">var</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A summary of the kernel can be obtained using the command <code class="docutils literal notranslate"><span class="pre">print</span> <span class="pre">k</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  <span class=" -Color -Color-Bold">rbf.       </span>  |  value  |  constraints  |  priors
  <span class=" -Color -Color-Bold">variance   </span>  |    1.0  |      +ve      |        
  <span class=" -Color -Color-Bold">lengthscale</span>  |    0.2  |      +ve      |        
</pre></div>
</div>
</div>
</div>
<p>It is also possible to plot the kernel as a function of one of its inputs (whilst fixing the other) with <code class="docutils literal notranslate"><span class="pre">k.plot()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">k</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_12_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_12_0.png" />
</div>
</div>
</div>
<div class="section" id="setting-covariance-function-parameters">
<h2>Setting Covariance Function Parameters<a class="headerlink" href="#setting-covariance-function-parameters" title="Permalink to this headline">¶</a></h2>
<p>The value of the covariance function parameters can be accessed and modified using <code class="docutils literal notranslate"><span class="pre">k['.*var']</span></code> where the string in bracket is a regular expression matching the parameter name as it appears in <code class="docutils literal notranslate"><span class="pre">print(k)</span></code>. Let’s use this to get an insight into the effect of the parameters on the shape of the covariance function.</p>
<p>We’ll now use it to set the lengthscale of the covariance to different values, and then plot the resulting covariance using the <code class="docutils literal notranslate"><span class="pre">k.plot()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>     <span class="c1"># By default, the parameters are set to 1.</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="c1">#k.lengthscale = t</span>
    <span class="n">k</span><span class="o">.</span><span class="n">variance</span> <span class="o">=</span> <span class="n">t</span>
    <span class="n">k</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">it</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;lengthscale&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_15_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_15_0.png" />
</div>
</div>
<div class="section" id="exercise-1">
<h3>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¶</a></h3>
<p>a) What is the effect of the lengthscale parameter on the covariance function?</p>
<p>b) First predict how the graphs will change if you replace <code class="docutils literal notranslate"><span class="pre">k.lengthscale</span></code> by <code class="docutils literal notranslate"><span class="pre">k.variance</span></code>, i.e., predict the effect of the variance parameter on the covariance function.  Then make the change and check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 1 b) answer</span>
<span class="c1"># insert revised code here (change the legend label as well)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="covariance-functions-in-gpy">
<h2>Covariance Functions in GPy<a class="headerlink" href="#covariance-functions-in-gpy" title="Permalink to this headline">¶</a></h2>
<p>Many covariance functions are already implemented in GPy. Instead of rbf, try constructing and plotting the following  covariance functions: <code class="docutils literal notranslate"><span class="pre">exponential</span></code>, <code class="docutils literal notranslate"><span class="pre">Matern32</span></code>, <code class="docutils literal notranslate"><span class="pre">Matern52</span></code>, <code class="docutils literal notranslate"><span class="pre">Brownian</span></code>, <code class="docutils literal notranslate"><span class="pre">linear</span></code>, <code class="docutils literal notranslate"><span class="pre">bias</span></code>,
<code class="docutils literal notranslate"><span class="pre">rbfcos</span></code>, <code class="docutils literal notranslate"><span class="pre">periodic_Matern32</span></code>, etc. Some of these covariance functions, such as <code class="docutils literal notranslate"><span class="pre">rbfcos</span></code>, are not
parametrized by a variance and a lengthscale. Furthermore, not all kernels are stationary (i.e., they can’t all be written as <span class="math notranslate nohighlight">\(k ( x, y) = f ( x − y)\)</span>, see for example the Brownian
covariance function). For plotting  so it may be interesting to change the value of the fixed input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kb1</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">PeriodicExponential</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                                   <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mf">6.28</span><span class="p">)</span>
<span class="n">kb2</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">StdPeriodic</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
                           <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mf">6.28</span><span class="p">)</span>
<span class="c1"># kb3 = GPy.kern.RBF(input_dim=1, variance=1.0, </span>
<span class="c1">#                                    lengthscale=1.0)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">kb1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">plot_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">ix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">kb2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">plot_limits</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">ix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="c1">#     kb3.plot(x,plot_limits=[0,5],ax=ax,color=f&quot;C{ix}&quot;,linestyle=&#39;:&#39;)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_23_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_23_0.png" />
</div>
</div>
</div>
<div class="section" id="computing-the-covariance-function-given-the-input-data-mathbf-x">
<h2>Computing the Covariance Function given the Input Data, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span><a class="headerlink" href="#computing-the-covariance-function-given-the-input-data-mathbf-x" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a <span class="math notranslate nohighlight">\(n\)</span> × <span class="math notranslate nohighlight">\(d\)</span> numpy array. Given a kernel <span class="math notranslate nohighlight">\(k\)</span>, the covariance matrix associated to
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is obtained with <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">k.K(X,X)</span></code>. The positive semi-definiteness of <span class="math notranslate nohighlight">\(k\)</span> ensures that <code class="docutils literal notranslate"><span class="pre">C</span></code>
is a positive semi-definite (psd) matrix regardless of the initial points <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. This can be
checked numerically by looking at the eigenvalues (the log of the eigenvalues is plotted below, so no
error means the eigenvalues are all positive):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern32</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>        <span class="c1"># 50*2 matrix of 2d standard Gaussians</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>                     <span class="c1"># covariance matrix</span>
<span class="n">eigvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>   <span class="c1"># Computes the eigenvalues of a matrix</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">eigvals</span><span class="p">))</span>
<span class="n">my_title</span> <span class="o">=</span> <span class="s1">&#39;Eigenvalues of the Matern 5/2 Covariance&#39;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;log10(eig)&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">my_title</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_26_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_26_0.png" />
</div>
</div>
<div class="section" id="follow-up-task">
<h3>Follow-up task<a class="headerlink" href="#follow-up-task" title="Permalink to this headline">¶</a></h3>
<p><em>Check this property for some other kernel and for a different set of points</em></p>
</div>
</div>
<div class="section" id="combining-covariance-functions">
<h2>Combining Covariance Functions<a class="headerlink" href="#combining-covariance-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="exercise-2">
<h3>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">¶</a></h3>
<p>a) A matrix, <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>, is positive semi-definite if the matrix inner product, <span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{K}\mathbf{x}\)</span> is greater than or equal to zero regardless of the values in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Given this it should be easy to see that the sum of two positive semi-definite matrices is also positive semi-definite. In the context of Gaussian processes, this is the sum of two covariance functions. What does this mean from a modelling perspective?</p>
<p><em>Hint</em>: there are actually two related interpretations for this. Think about the properties of a Gaussian distribution, and in what situation the sum of Gaussian variances arises.</p>
<p>b) What about the element-wise product of two covariance functions? In other words if we define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
k(\mathbf{x}, \mathbf{x}^\prime) = k_1(\mathbf{x}, \mathbf{x}^\prime) \, k_2(\mathbf{x}, \mathbf{x}^\prime)
\end{align*}\]</div>
<p>then is <span class="math notranslate nohighlight">\(k(\mathbf{x}, \mathbf{x}^\prime)\)</span> a valid covariance function?</p>
</div>
<div class="section" id="combining-covariance-functions-in-gpy">
<h3>Combining Covariance Functions in GPy<a class="headerlink" href="#combining-covariance-functions-in-gpy" title="Permalink to this headline">¶</a></h3>
<p>In GPy you can easily combine covariance functions you have created using the sum and product operators, <code class="docutils literal notranslate"><span class="pre">+</span></code> and <code class="docutils literal notranslate"><span class="pre">*</span></code>. So, for example, if we wish to combine an exponentiated quadratic covariance with a Matern 5/2 then we can write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kern1</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">kern2</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>


<span class="c1">#kern1 = GPy.kern.Linear(1)</span>
<span class="c1">#kern2 = GPy.kern.Linear(1)</span>
<span class="n">kern</span> <span class="o">=</span> <span class="n">kern1</span> <span class="o">+</span> <span class="n">kern2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kern</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">kern</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  <span class=" -Color -Color-Bold">sum.             </span>  |  value  |  constraints  |  priors
  <span class=" -Color -Color-Bold">rbf.variance     </span>  |    1.0  |      +ve      |        
  <span class=" -Color -Color-Bold">rbf.lengthscale  </span>  |    2.0  |      +ve      |        
  <span class=" -Color -Color-Bold">Mat52.variance   </span>  |    2.0  |      +ve      |        
  <span class=" -Color -Color-Bold">Mat52.lengthscale</span>  |    0.1  |      +ve      |        
</pre></div>
</div>
<img alt="../../_images/Gaussian_processes_exercises-Copy4_39_1.png" src="../../_images/Gaussian_processes_exercises-Copy4_39_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define X to be 500 points evenly spaced over [0,1]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> 
<span class="c1"># reshape X to make it n*p --- GPy uses &#39;design matrices&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span>    <span class="c1"># now 500 x 1  </span>

<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># vector of the means --- we could use a mean function here, but here it is just zero.</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c1"># compute the covariance matrix associated with inputs X</span>

<span class="c1"># Generate &#39;nsamples&#39; separate samples paths from a Gaussian with mean mu and covariance C</span>
<span class="n">nsamples</span><span class="o">=</span><span class="mi">10</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:],</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,:]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_40_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_40_0.png" />
</div>
</div>
<p>Or if we wanted to multiply them we can write</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kern</span> <span class="o">=</span> <span class="n">kern1</span><span class="o">*</span><span class="n">kern2</span>

<span class="n">kern3</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">variances</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="n">kern_cube</span> <span class="o">=</span> <span class="n">kern3</span><span class="o">*</span><span class="n">kern2</span><span class="o">*</span><span class="n">kern1</span>

<span class="nb">print</span><span class="p">(</span><span class="n">kern</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">kern</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  <span class=" -Color -Color-Bold">mul.             </span>  |  value  |  constraints  |  priors
  <span class=" -Color -Color-Bold">rbf.variance     </span>  |    1.0  |      +ve      |        
  <span class=" -Color -Color-Bold">rbf.lengthscale  </span>  |    2.0  |      +ve      |        
  <span class=" -Color -Color-Bold">Mat52.variance   </span>  |    2.0  |      +ve      |        
  <span class=" -Color -Color-Bold">Mat52.lengthscale</span>  |    0.1  |      +ve      |        
</pre></div>
</div>
<img alt="../../_images/Gaussian_processes_exercises-Copy4_42_1.png" src="../../_images/Gaussian_processes_exercises-Copy4_42_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define X to be 500 points evenly spaced over [0,1]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> 
<span class="c1"># reshape X to make it n*p --- GPy uses &#39;design matrices&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span>    <span class="c1"># now 500 x 1  </span>

<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># vector of the means --- we could use a mean function here, but here it is just zero.</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">kern</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c1"># compute the covariance matrix associated with inputs X</span>
<span class="n">C_cube</span> <span class="o">=</span> <span class="n">kern_cube</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Generate &#39;nsamples&#39; separate samples paths from a Gaussian with mean mu and covariance C</span>
<span class="n">nsamples</span><span class="o">=</span><span class="mi">10</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">Zp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C_cube</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">axp</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:],</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,:]);</span>
    <span class="n">axp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:],</span><span class="n">Zp</span><span class="p">[</span><span class="n">i</span><span class="p">,:]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_43_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_43_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="sampling-from-a-gaussian-process">
<h2>2 Sampling from a Gaussian Process<a class="headerlink" href="#sampling-from-a-gaussian-process" title="Permalink to this headline">¶</a></h2>
<p>The Gaussian process provides a prior over an infinite dimensional function. It is defined by a covariance <em>function</em> and a mean <em>function</em>. When we compute the covariance matrix using <code class="docutils literal notranslate"><span class="pre">kern.K(X,</span> <span class="pre">X)</span></code> we are computing a covariance <em>matrix</em> between the values of the function that correspond to the input locations in the matrix <code class="docutils literal notranslate"><span class="pre">X</span></code>. If we want to have a look at the type of functions that arise from a particular Gaussian process we can never generate all values of the function, because there are infinite values. However, we can generate samples from a Gaussian <em>distribution</em> based on a covariance matrix associated with a particular matrix of input locations <code class="docutils literal notranslate"><span class="pre">X</span></code>. If these locations are chosen appropriately then they give us a good idea of the underlying function. For example, for a one dimensional function, if we choose <code class="docutils literal notranslate"><span class="pre">X</span></code> to be uniformly spaced across part of the real line, and the spacing is small enough, we’ll get an idea of the underlying function. We will now use this trick to draw sample paths from a Gaussian process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># define X to be 500 points evenly spaced over [0,1]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> 
<span class="c1"># reshape X to make it n*p --- GPy uses &#39;design matrices&#39;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span>    <span class="c1"># now 500 x 1  </span>

<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># vector of the means --- we could use a mean function here, but here it is just zero.</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="c1"># compute the covariance matrix associated with inputs X</span>

<span class="c1"># Generate &#39;nsamples&#39; separate samples paths from a Gaussian with mean mu and covariance C</span>
<span class="n">nsamples</span><span class="o">=</span><span class="mi">10</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:],</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,:]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_46_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_46_0.png" />
</div>
</div>
<p>Our choice of <code class="docutils literal notranslate"><span class="pre">X</span></code> means that the points are close enough together to look like functions. We can see the structure of the covariance matrix we are plotting from if we visualize C.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">C</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_48_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_48_0.png" />
</div>
</div>
<p><em>Predict what the plot will look like for <code class="docutils literal notranslate"><span class="pre">nsamples=50</span></code> and then <code class="docutils literal notranslate"><span class="pre">nsamples=500</span></code>.  Try it!</em></p>
<p><em>Predict what will happen if you replace <br>
<code class="docutils literal notranslate"><span class="pre">mu</span> <span class="pre">=</span> <span class="pre">np.zeros(len(X))</span></code> <br>
by <br>
<code class="docutils literal notranslate"><span class="pre">mu</span> <span class="pre">=</span> <span class="pre">np.linspace(-1.,</span> <span class="pre">1.,</span> <span class="pre">len(X))</span></code> <br>
and then do it.</em></p>
<div class="section" id="exercise-3">
<h3>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this headline">¶</a></h3>
<p><em>Modify the code below so that it plots the sampled paths from the nine different covariance structures that are generated.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">kerns</span> <span class="o">=</span> <span class="p">[</span><span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern32</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
         <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Matern52</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Brownian</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Bias</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
         <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">PeriodicExponential</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> 
         <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">White</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span> 
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="kc">None</span><span class="p">]</span> 

<span class="c1"># We could use a mean function here, but here it is just zero.</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># vector of the means </span>

<span class="n">nsamples</span><span class="o">=</span><span class="mi">10</span>    
    
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kerns</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>   <span class="c1"># compute the covariance matrix associated with inputs X</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="n">nsamples</span><span class="p">)</span>
    <span class="n">a</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">))</span>
    <span class="c1"># Fill in code here using previous code as a prototype:</span>
    <span class="c1">#  we need to plot the nsamples on the current axis a</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
        <span class="n">a</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:],</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,:]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_53_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_53_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="a-gaussian-process-regression-model">
<h2>3 A Gaussian Process Regression Model<a class="headerlink" href="#a-gaussian-process-regression-model" title="Permalink to this headline">¶</a></h2>
<p>We will now combine the Gaussian process prior with some data to form a GP regression model with GPy. We will generate data from the function <span class="math notranslate nohighlight">\(f ( x ) = − \cos(\pi x ) + \sin(4\pi x )\)</span> over <span class="math notranslate nohighlight">\([0, 1]\)</span>, adding some noise to give <span class="math notranslate nohighlight">\(y(x) = f(x) + \epsilon\)</span>, with the noise being Gaussian distributed, <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, 0.01)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Yfunc</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True function for GP regression model calibration.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> 

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mi">10</span><span class="p">)[:,</span><span class="kc">None</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Yfunc</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> 

<span class="c1"># Use many points over a larger range for the true arrays</span>
<span class="n">Xtrue</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">Ytrue</span> <span class="o">=</span> <span class="n">Yfunc</span><span class="p">(</span><span class="n">Xtrue</span><span class="p">)</span>
    
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrue</span><span class="p">,</span> <span class="n">Ytrue</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s1">&#39;kx&#39;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mf">1.5</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.0, 1.0)
</pre></div>
</div>
<img alt="../../_images/Gaussian_processes_exercises-Copy4_57_1.png" src="../../_images/Gaussian_processes_exercises-Copy4_57_1.png" />
</div>
</div>
<p>A GP regression model based on an exponentiated quadratic covariance function can be defined by first defining a covariance function,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And then combining it with the data to form a Gaussian process model,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">GPRegression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just as for the covariance function object, we can find out about the model using the command <code class="docutils literal notranslate"><span class="pre">print(m)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Name : GP regression
Objective : 13.441491190694581
Number of Parameters : 3
Number of Optimization Parameters : 3
Updates : True
Parameters:
  <span class=" -Color -Color-Bold">GP_regression.         </span>  |  value  |  constraints  |  priors
  <span class=" -Color -Color-Bold">rbf.variance           </span>  |    1.0  |      +ve      |        
  <span class=" -Color -Color-Bold">rbf.lengthscale        </span>  |    1.0  |      +ve      |        
  <span class=" -Color -Color-Bold">Gaussian_noise.variance</span>  |    1.0  |      +ve      |        
</pre></div>
</div>
</div>
</div>
<p>Note that by default the model includes some observation noise
with variance 1. We can see the posterior mean prediction and visualize the marginal posterior variances using <code class="docutils literal notranslate"><span class="pre">m.plot()</span></code>.</p>
<p><strong>Note:</strong> The <code class="docutils literal notranslate"><span class="pre">plot</span></code> command shows the mean of the GP model as well as the 95% confidence region.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_65_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_65_0.png" />
</div>
</div>
<p>The actual predictions of the model for a set of points <code class="docutils literal notranslate"><span class="pre">Xnew</span></code> can be computed using</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Ynew</span><span class="p">,</span> <span class="n">Yvar</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">Xnew</span></code> is an <span class="math notranslate nohighlight">\(m \times p\)</span> array, where <span class="math notranslate nohighlight">\(m\)</span> is the number of points that we want to predict and <span class="math notranslate nohighlight">\(p\)</span> is the dimensionality of the parameter space.</p>
<p>We can also extract the predictive quantiles around the prediction at <code class="docutils literal notranslate"><span class="pre">Xnew</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Ylo95</span><span class="p">,</span> <span class="n">Yhi95</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict_quantiles</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xnew</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)[:,</span><span class="kc">None</span><span class="p">]</span>
<span class="p">(</span><span class="n">Ymean</span><span class="p">,</span><span class="n">Yvar</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>

<span class="c1"># quantiles=(2.5, 97.5) are default</span>
<span class="p">(</span><span class="n">Ylo95</span><span class="p">,</span> <span class="n">Yhi95</span><span class="p">)</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">predict_quantiles</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;  X   mean Y   var Y   lo95</span><span class="si">%   hi</span><span class="s2">95%&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="p">(</span><span class="n">Xi</span><span class="p">,</span> <span class="n">Ymeani</span><span class="p">,</span> <span class="n">Yvari</span><span class="p">,</span> <span class="n">Yloi</span><span class="p">,</span> <span class="n">Yhii</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">Xnew</span><span class="p">,</span> <span class="n">Ymean</span><span class="p">,</span> <span class="n">Yvar</span><span class="p">,</span> <span class="n">Ylo95</span><span class="p">,</span> <span class="n">Yhi95</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Xi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">  </span><span class="si">{</span><span class="n">Ymeani</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">Yvari</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">   &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Yloi</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="n">Yhii</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">5.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  X   mean Y   var Y   lo95%   hi95%
 0.00  -0.30    1.20   -2.45    1.85
 0.50  -0.01    1.10   -2.06    2.05
 1.00   0.28    1.20   -1.87    2.43
 1.50   0.40    1.48   -1.99    2.78
</pre></div>
</div>
</div>
</div>
<div class="section" id="exercise-4">
<h3>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this headline">¶</a></h3>
<p>a) What do you think about this first fit? Does the prior given by the GP seem to be
adapted?</p>
<p>b) The parameters of the models can be modified using a regular expression matching the parameters names (for example <code class="docutils literal notranslate"><span class="pre">m['Gaussian_noise.variance']</span> <span class="pre">=</span> <span class="pre">0.001</span></code> ). Change the values of the parameters to obtain a better fit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 4 b) </span>
<span class="c1"># make a plot for a better fit here</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># KEY</span>
<span class="n">m</span><span class="p">[</span><span class="s1">&#39;Gaussian_noise.variance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">m</span><span class="p">[</span><span class="s1">&#39;rbf.variance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">m</span><span class="p">[</span><span class="s1">&#39;rbf.lengthscale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrue</span><span class="p">,</span> <span class="n">Ytrue</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c1">#ax.set_xlim(-0.4, 1.4)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;dataplot&#39;: [&lt;matplotlib.collections.PathCollection at 0x15b18a190&gt;],
 &#39;gpmean&#39;: [[&lt;matplotlib.lines.Line2D at 0x15a3cdf90&gt;]],
 &#39;gpconfidence&#39;: [&lt;matplotlib.collections.PolyCollection at 0x15a405450&gt;]}
</pre></div>
</div>
<img alt="../../_images/Gaussian_processes_exercises-Copy4_73_1.png" src="../../_images/Gaussian_processes_exercises-Copy4_73_1.png" />
</div>
</div>
<p>c) As in Section 2, random sample paths from the conditional GP can be obtained using
<code class="docutils literal notranslate"><span class="pre">np.random.multivariate_normal(mu[:,0],C)</span></code> where the mean vector and covariance
matrix <code class="docutils literal notranslate"><span class="pre">mu</span></code>, <code class="docutils literal notranslate"><span class="pre">C</span></code> are obtained through the predict function <code class="docutils literal notranslate"><span class="pre">mu,</span> <span class="pre">C</span> <span class="pre">=</span> <span class="pre">m.predict(Xp,full_cov=True)</span></code>. Obtain 20 samples from the posterior sample and plot them alongside the data below. Compare the random sample paths to the 95% confidence region that is shown with the <code class="docutils literal notranslate"><span class="pre">m.plot()</span></code> command.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 4 c) answer</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="covariance-function-parameter-estimation">
<h3>Covariance Function Parameter Estimation<a class="headerlink" href="#covariance-function-parameter-estimation" title="Permalink to this headline">¶</a></h3>
<p>The kernel parameter values can be estimated by maximizing the <em>likelihood</em> of the observations. Since we don’t want one of the variances to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">constrain_positive</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>reconstraining parameters GP_regression
</pre></div>
</div>
</div>
</div>
<p>The warnings are because the parameters are already constrained by default, the software is warning us that they are being reconstrained.</p>
<p>Now we can optimize the model using the <code class="docutils literal notranslate"><span class="pre">m.optimize()</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#m[&#39;rbf.lengthscale&#39;] = 0.1</span>
<span class="n">m</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xtrue</span><span class="p">,</span> <span class="n">Ytrue</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Name : GP regression
Objective : 6.020187741234467
Number of Parameters : 3
Number of Optimization Parameters : 3
Updates : True
Parameters:
  <span class=" -Color -Color-Bold">GP_regression.         </span>  |                   value  |  constraints  |  priors
  <span class=" -Color -Color-Bold">rbf.variance           </span>  |       3.844008245027275  |      +ve      |        
  <span class=" -Color -Color-Bold">rbf.lengthscale        </span>  |     0.19251725818225146  |      +ve      |        
  <span class=" -Color -Color-Bold">Gaussian_noise.variance</span>  |  2.2021336916011984e-12  |      +ve      |        
</pre></div>
</div>
<img alt="../../_images/Gaussian_processes_exercises-Copy4_81_1.png" src="../../_images/Gaussian_processes_exercises-Copy4_81_1.png" />
</div>
</div>
<p>The parameters obtained after optimisation can be compared with the values selected by hand above. As previously, you can modify the kernel used for building the model to investigate its influence on the model.</p>
</div>
</div>
<div class="section" id="a-running-example">
<h2>4 A Running Example<a class="headerlink" href="#a-running-example" title="Permalink to this headline">¶</a></h2>
<p>Now we’ll consider a small example with real world data, data giving the pace of all marathons run at the olympics. To load the data use</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">GPy</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">authorize_download</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">True</span> 
                          <span class="c1"># prevents requesting authorization for download.</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">olympic_marathon_men</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;details&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Olympic mens&#39; marathon gold medal winning times from 1896 to 2012. Time given in pace (minutes per kilometer). Data is originally downloaded and collated from Wikipedia, we are not responsible for errors in the data
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Y&#39;</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;year&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;marathon pace min/km&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Gaussian_processes_exercises-Copy4_86_0.png" src="../../_images/Gaussian_processes_exercises-Copy4_86_0.png" />
</div>
</div>
<div class="section" id="exercise-5">
<h3>Exercise 5<a class="headerlink" href="#exercise-5" title="Permalink to this headline">¶</a></h3>
<p>a) Build a Gaussian process model for the olympic data set using a combination of an exponentiated quadratic and a bias covariance function. Fit the covariance function parameters and the noise to the data. Plot the fit and error bars from 1870 to 2030. Do you think the predictions are reasonable? If not why not?</p>
<p>Compute also the log likelihood for the optimum.</p>
<p><em>Hint:</em> use <code class="docutils literal notranslate"><span class="pre">model.log_likelihood()</span></code> for computing the log likelihood.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 5 a) answer </span>
<span class="c1"># Enter code here</span>
</pre></div>
</div>
</div>
</div>
<p>b) Fit the same model, but this time initialize the length scale of the exponentiated quadratic to 0.5. What has happened? Which model has the higher log likelihood, this one or the one from (a)?</p>
<p><em>Hint:</em> use <code class="docutils literal notranslate"><span class="pre">model.log_likelihood()</span></code> for computing the log likelihood.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 5 b) answer</span>
<span class="c1"># Enter code here</span>
</pre></div>
</div>
</div>
</div>
<p>c) Modify your model by including two covariance functions. Initialize a covariance function with an exponentiated quadratic part, a Matern 3/2 part and a bias covariance. Set the initial length scale of the exponentiated quadratic to 80 years, set the initial length scale of the Matern 3/2 to 10 years. Optimize the new model and plot the fit again. How does it compare with the previous model?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 5 c) answer</span>
<span class="c1"># Enter code here</span>
</pre></div>
</div>
</div>
</div>
<p>d) Repeat part c) but now initialize both of the covariance functions’ length scales to 20 years. Check the model parameters, what happens now?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 5 d) answer</span>
<span class="c1"># Enter code here</span>
</pre></div>
</div>
</div>
</div>
<p>e) Now model the data with a product of an exponentiated quadratic covariance function and a linear covariance function. Fit the covariance function parameters. Why are the variance parameters of the linear part so small? How could this be fixed?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise 5 e) answer</span>
<span class="c1"># Enter code here</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "furnstahl/Physics-8820",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-8805-env-py"
        },
        kernelOptions: {
            kernelName: "conda-env-8805-env-py",
            path: "./notebooks/Gaussian_processes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-8805-env-py'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Dick Furnstahl<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>